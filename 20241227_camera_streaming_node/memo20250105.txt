共有する


あなた:
ubuntu20.04 で c++ programを作成してください
ros2 foxy nodeを作成して、30hzの周期でタイマーを作成し、OpenCVのVideoCapture(gstreamerのpipelineを使う)でudp h.264 streamingから1フレーム受信し、それを共有メモリにコピーしてください
共有メモリは以下のような構造にしてください
#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080
struct
{
unsigned long serial_number;
int width;
int height;
unsigned char data[MAX_WIDTH][MAX_HEIGHT];
};
共有メモリへの書き込み時にはプロセス間排他制御を行ってください
録画フラグを判断して、フレームをOpenCVのVideoWriterでmp4ファイルに録画してください
画面キャプチャ要求があれば、現在の１フレームをjpegエンコードしてjpgファイルに保存してください
録画ファイルはoutput_directory+"/Video/"の下に現在日付時刻yyyymmdd-MMDDSS.999（999はミリ秒）をファイル名にして.mp4を拡張子にして保存してください
キャプチャファイルはoutput_directory+"/Photo/"の下に現在日付時刻yyyymmdd-MMDDSS.999（999はミリ秒）をファイル名にして.jpgを拡張子にして保存してください
以下の３つのros2 serviceを提供してください
	- udp h.264 streamingの受信の開始/終了（streamingフラグのon/off）	SetBool
	- udp h.264 streamingの録画の開始/終了（録画フラグのon/off）	SetBool
	- udp h.264 streamingの現在フレームをjpegで保存してください	Trigger
ChatGPT:

このC++プログラムでは、ROS2 Foxyを使用し、GStreamerパイプラインを通じてUDP H.264ストリーミングからフレームを受信し、それを共有メモリにコピーします。また、録画フラグをもとにフレームをMP4ファイルに保存したり、キャプチャ要求があればJPEGとして保存する機能も実装します。

全体の構造を以下のように設計します：

ROS2ノードの作成：サービスを介してストリーミング、録画、キャプチャの制御を行う。
OpenCVを使用してUDP H.264ストリーミングを受信：GStreamerを使ってcv::VideoCaptureでストリームを取得。
共有メモリの管理：プロセス間排他制御（mutex）を使ってフレームを共有メモリにコピー。
録画とキャプチャ機能：OpenCVのVideoWriterで録画し、JPEGエンコードでキャプチャを保存。
サービスの実装：ROS2のサービスでストリーミング、録画、キャプチャの要求を受け取る。
以下は、各機能を実装するためのサンプルコードです。

1. CMakeLists.txtとpackage.xmlの設定
CMakeLists.txt
cmake
コードをコピーする
cmake_minimum_required(VERSION 3.5)
project(video_stream_node)

find_package(ament_cmake REQUIRED)
find_package(rclcpp REQUIRED)
find_package(std_srvs REQUIRED)
find_package(sensor_msgs REQUIRED)
find_package(OpenCV REQUIRED)

add_executable(video_stream_node src/video_stream_node.cpp)
ament_target_dependencies(video_stream_node rclcpp std_srvs OpenCV)

install(TARGETS
  video_stream_node
  DESTINATION lib/${PROJECT_NAME})

ament_package()
package.xml
xml
コードをコピーする
<?xml version="1.0"?>
<package format="2">
  <name>video_stream_node</name>
  <version>0.0.1</version>
  <description>ROS2 Node for video streaming with OpenCV</description>

  <maintainer email="you@yourdomain.com">Your Name</maintainer>
  <license>Apache 2.0</license>

  <buildtool_depend>ament_cmake</buildtool_depend>
  <depend>rclcpp</depend>
  <depend>std_srvs</depend>
  <depend>sensor_msgs</depend>
  <depend>opencv</depend>
  
</package>
2. ノードの実装
src/video_stream_node.cpp

cpp
コードをコピーする
#include <rclcpp/rclcpp.hpp>
#include <std_srvs/srv/set_bool.hpp>
#include <std_srvs/srv/trigger.hpp>
#include <opencv2/opencv.hpp>
#include <opencv2/videoio.hpp>
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#include <chrono>
#include <thread>
#include <mutex>
#include <ctime>
#include <iomanip>

#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080

struct SharedMemory {
  unsigned long serial_number;
  int width;
  int height;
  unsigned char data[MAX_HEIGHT][MAX_WIDTH];
};

class VideoStreamNode : public rclcpp::Node {
public:
  VideoStreamNode() : Node("video_stream_node"), streaming_(false), recording_(false) {
    // ROS2 Services
    stream_service_ = this->create_service<std_srvs::srv::SetBool>(
      "set_streaming", std::bind(&VideoStreamNode::handle_streaming, this, std::placeholders::_1, std::placeholders::_2));
    record_service_ = this->create_service<std_srvs::srv::SetBool>(
      "set_recording", std::bind(&VideoStreamNode::handle_recording, this, std::placeholders::_1, std::placeholders::_2));
    capture_service_ = this->create_service<std_srvs::srv::Trigger>(
      "capture_frame", std::bind(&VideoStreamNode::handle_capture, this, std::placeholders::_1, std::placeholders::_2));

    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_CREAT | O_RDWR, 0666);
    ftruncate(shm_fd_, sizeof(SharedMemory));
    shared_memory_ = (SharedMemory*) mmap(0, sizeof(SharedMemory), PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
      RCLCPP_ERROR(this->get_logger(), "Shared memory initialization failed.");
      exit(EXIT_FAILURE);
    }

    shared_memory_->serial_number = 0;
    shared_memory_->width = MAX_WIDTH;
    shared_memory_->height = MAX_HEIGHT;

    // GStreamer pipeline for H.264 UDP streaming
    std::string pipeline = "udpsrc port=5000 ! application/x-rtp, payload=96 ! "
                           "rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink";
    cap_.open(pipeline, cv::CAP_GSTREAMER);
    if (!cap_.isOpened()) {
      RCLCPP_ERROR(this->get_logger(), "Unable to open video stream.");
      exit(EXIT_FAILURE);
    }

    // Timer to fetch frames at 30 Hz
    timer_ = this->create_wall_timer(
      std::chrono::milliseconds(33), std::bind(&VideoStreamNode::timer_callback, this));
  }

  ~VideoStreamNode() {
    munmap(shared_memory_, sizeof(SharedMemory));
    shm_unlink("/video_shm");
  }

private:
  void handle_streaming(const std::shared_ptr<std_srvs::srv::SetBool::Request> request,
                        std::shared_ptr<std_srvs::srv::SetBool::Response> response) {
    streaming_ = request->data;
    response->success = true;
    response->message = streaming_ ? "Streaming started" : "Streaming stopped";
  }

  void handle_recording(const std::shared_ptr<std_srvs::srv::SetBool::Request> request,
                        std::shared_ptr<std_srvs::srv::SetBool::Response> response) {
    recording_ = request->data;
    if (recording_) {
      // Create VideoWriter for recording
      std::string timestamp = get_current_time_string();
      std::string filename = output_directory_ + "/Video/" + timestamp + ".mp4";
      writer_.open(filename, cv::VideoWriter::fourcc('M', 'P', '4', 'V'), 30, cv::Size(MAX_WIDTH, MAX_HEIGHT));
    } else {
      writer_.release();
    }
    response->success = true;
    response->message = recording_ ? "Recording started" : "Recording stopped";
  }

  void handle_capture(const std::shared_ptr<std_srvs::srv::Trigger::Request> request,
                      std::shared_ptr<std_srvs::srv::Trigger::Response> response) {
    std::string timestamp = get_current_time_string();
    std::string filename = output_directory_ + "/Photo/" + timestamp + ".jpg";
    cv::imwrite(filename, current_frame_);
    response->success = true;
    response->message = "Captured frame saved as " + filename;
  }

  void timer_callback() {
    if (!streaming_) return;

    std::lock_guard<std::mutex> lock(mutex_);

    cap_ >> current_frame_;
    if (current_frame_.empty()) return;

    // Copy to shared memory
    shared_memory_->serial_number++;
    shared_memory_->width = current_frame_.cols;
    shared_memory_->height = current_frame_.rows;
    memcpy(shared_memory_->data, current_frame_.data, current_frame_.total());

    // Record the frame if recording is active
    if (recording_ && writer_.isOpened()) {
      writer_ << current_frame_;
    }
  }

  std::string get_current_time_string() {
    auto now = std::chrono::system_clock::now();
    auto now_time_t = std::chrono::system_clock::to_time_t(now);
    auto now_ms = std::chrono::duration_cast<std::chrono::milliseconds>(now.time_since_epoch()) % 1000;

    std::ostringstream timestamp;
    timestamp << std::put_time(std::localtime(&now_time_t), "%Y%m%d-%H%M%S")
              << '.' << std::setw(3) << std::setfill('0') << now_ms.count();
    return timestamp.str();
  }

  int shm_fd_;
  SharedMemory* shared_memory_;
  std::mutex mutex_;
  bool streaming_;
  bool recording_;
  cv::Mat current_frame_;
  cv::VideoCapture cap_;
  cv::VideoWriter writer_;
  rclcpp::TimerBase::SharedPtr timer_;
  rclcpp::Service<std_srvs::srv::SetBool>::SharedPtr stream_service_;
  rclcpp::Service<std_srvs::srv::SetBool>::SharedPtr record_service_;
  rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr capture_service_;
  std::string output_directory_ = "/your/output/directory";
};

int main(int argc, char *argv[]) {
  rclcpp::init(argc, argv);
  rclcpp::spin(std::make_shared<VideoStreamNode>());
  rclcpp::shutdown();
  return 0;
}
説明
共有メモリ：mmapで共有メモリを作成し、SharedMemory構造体を使用します。フレームデータは排他制御を行いながら書き込みます。
GStreamerパイプライン：cv::VideoCaptureを使い、UDP H.264ストリーミングを受信します。
サービス：SetBoolサービスでストリーミングや録画の開始/終了を制御し、Triggerサービスで現在のフレームをJPEGとして保存します。
録画とキャプチャ：VideoWriterを使い、録画フラグが有効な場合にMP4形式で保存し、JPEGファイルとしてフレームをキャプチャします。
実行方法
CMakeLists.txtとpackage.xmlを用意して、ビルドします。
bash
コードをコピーする
colcon build --packages-select video_stream_node
source install/setup.bash
ノードを起動し、サービスを呼び出します。
bash
コードをコピーする
ros2 run video_stream_node video_stream_node
サービスの呼び出し例
ストリーミング開始：
bash
コードをコピーする
ros2 service call /set_streaming std_srvs/srv/SetBool "{data: true}"
録画開始：
bash
コードをコピーする
ros2 service call /set_recording std_srvs/srv/SetBool "{data: true}"
キャプチャ：
bash
コードをコピーする
ros2 service call /capture_frame std_srvs/srv/Trigger




あなた:
上のprogramが書き込んだ共有メモリのimage data を３０ｈｚの頻度で読み込んで表示する、別のqt5 c++ programを作成してください
ChatGPT:

別のQt5 C++プログラムで、先に作成した共有メモリから画像データを30Hzの頻度で読み込み、表示する方法を説明します。このプログラムは、共有メモリに書き込まれたフレームを読み込み、表示する機能を持つQtウィジェットを使います。

1. Qt5 プロジェクトの設定
CMakeLists.txt
cmake
コードをコピーする
cmake_minimum_required(VERSION 3.5)
project(shared_memory_viewer)

set(CMAKE_CXX_STANDARD 14)

find_package(Qt5 REQUIRED COMPONENTS Widgets)
find_package(OpenCV REQUIRED)

add_executable(shared_memory_viewer src/main.cpp src/viewer_widget.cpp)
target_link_libraries(shared_memory_viewer Qt5::Widgets ${OpenCV_LIBS})

install(TARGETS shared_memory_viewer DESTINATION bin)
project.pro (Qt Creator用)
もしCMakeLists.txtではなく、Qt Creatorを使用している場合は、project.proを使います。

pro
コードをコピーする
QT += widgets
CONFIG += c++14
SOURCES += main.cpp \
           viewer_widget.cpp
HEADERS += viewer_widget.h
INCLUDEPATH += /usr/include/opencv4
LIBS += `pkg-config --libs opencv4`
2. viewer_widget.h の作成
共有メモリからフレームを読み込み、画像を表示するためのウィジェットを定義します。

cpp
コードをコピーする
#ifndef VIEWER_WIDGET_H
#define VIEWER_WIDGET_H

#include <QWidget>
#include <QTimer>
#include <opencv2/opencv.hpp>
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#include <mutex>

#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080

struct SharedMemory {
    unsigned long serial_number;
    int width;
    int height;
    unsigned char data[MAX_HEIGHT][MAX_WIDTH];
};

class ViewerWidget : public QWidget {
    Q_OBJECT

public:
    explicit ViewerWidget(QWidget *parent = nullptr);
    ~ViewerWidget();

protected:
    void paintEvent(QPaintEvent *event) override;

private slots:
    void updateFrame();

private:
    QTimer *timer_;
    cv::Mat current_frame_;
    int shm_fd_;
    SharedMemory *shared_memory_;
    std::mutex mutex_;
};

#endif // VIEWER_WIDGET_H
3. viewer_widget.cpp の実装
このファイルでは、30Hzの頻度で共有メモリからデータを読み込み、ウィジェットに描画します。

cpp
コードをコピーする
#include "viewer_widget.h"
#include <QPainter>
#include <QImage>
#include <QTimer>

ViewerWidget::ViewerWidget(QWidget *parent) : QWidget(parent) {
    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_RDONLY, 0666);
    if (shm_fd_ == -1) {
        throw std::runtime_error("Failed to open shared memory.");
    }

    shared_memory_ = (SharedMemory*)mmap(0, sizeof(SharedMemory), PROT_READ, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
        throw std::runtime_error("Failed to map shared memory.");
    }

    // Timer for updating the frame at 30Hz
    timer_ = new QTimer(this);
    connect(timer_, &QTimer::timeout, this, &ViewerWidget::updateFrame);
    timer_->start(33);  // 30Hz = 1000ms / 30 = 33ms
}

ViewerWidget::~ViewerWidget() {
    munmap(shared_memory_, sizeof(SharedMemory));
    close(shm_fd_);
}

void ViewerWidget::updateFrame() {
    std::lock_guard<std::mutex> lock(mutex_);

    // Copy data from shared memory to OpenCV Mat
    cv::Mat frame(shared_memory_->height, shared_memory_->width, CV_8UC1, (void*)shared_memory_->data);
    cv::cvtColor(frame, current_frame_, cv::COLOR_GRAY2RGB);  // Convert grayscale to RGB (if needed)

    // Update the display
    update();
}

void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_frame_.empty()) return;

    // Convert OpenCV Mat to QImage
    QImage image(current_frame_.data, current_frame_.cols, current_frame_.rows, current_frame_.step, QImage::Format_RGB888);

    // Paint the image onto the widget
    QPainter painter(this);
    painter.drawImage(0, 0, image.scaled(this->size(), Qt::KeepAspectRatio));
}
4. main.cpp の実装
Qtアプリケーションを初期化し、ウィジェットを表示するコードです。

cpp
コードをコピーする
#include <QApplication>
#include "viewer_widget.h"

int main(int argc, char *argv[]) {
    QApplication app(argc, argv);

    ViewerWidget viewer;
    viewer.resize(1920, 1080);  // Set initial size to Full HD
    viewer.show();

    return app.exec();
}
説明
共有メモリへのアクセス：

shm_openを使い、共有メモリセグメントを開きます。
mmapで共有メモリをマッピングし、SharedMemory構造体のポインタを作成します。
メモリがマッピングされた後、30Hzのタイマーを使って定期的に共有メモリからフレームを読み込みます。
Qtウィジェットでの表示：

QPainterを使って、OpenCVのcv::MatからQImageに変換したフレームを描画します。
ウィジェットのリサイズに対応し、drawImageで描画サイズを変更します。
更新処理：

QTimerを使い、33ミリ秒ごとにupdateFrame()が呼び出され、フレームが更新されます。そこで共有メモリから新しいフレームを取得して表示します。
paintEvent：

Qtの描画メソッドpaintEventで、画面にフレームを描画します。これはウィジェットが再描画されるたびに呼び出されます。
実行手順
Qtプロジェクトをビルドします。
bash
コードをコピーする
mkdir build
cd build
cmake ..
make
プログラムを実行します。
bash
コードをコピーする
./shared_memory_viewer
これで、Qtウィジェットに30Hzで共有メモリに書き込まれたフレームが表示されるはずです。

メモ
このプログラムは共有メモリに正しいデータが書き込まれていることが前提です。VideoCaptureが正しく共有メモリにフレームをコピーしていれば、こちらで正しく表示できます。
描画部分では、フレームのカラー形式に応じてcvtColorなどで適切な変換を行う必要があります。もしフレームがRGBではない場合、適切な形式に変換してください。




あなた:
上の２つのc++ programを以下の通り修正してください
共有メモリへのimage data書き込み側のc++ programでは、そのimage dataをQImage::Format_RGB888形式ですぐにQImageをつくることができるようなimage dataで書き込んでください
共有メモリの読み込み側では、OpenCVを使わず、image dataをそのままQImage::Format_RGB888形式でQImageを作成して表示してください
ChatGPT:

要求に基づき、2つのC++プログラムを修正していきます。

書き込み側プログラムの修正: 共有メモリに書き込む画像データを、QImage::Format_RGB888形式で書き込みます。
読み込み側プログラムの修正: 画像データをそのままQImage形式に変換して表示し、OpenCVを使わない形にします。
1. 書き込み側（ROS2ノード）の修正
まず、QImage::Format_RGB888形式に合わせた画像データを共有メモリに書き込むようにします。変更点は、OpenCVで受け取った画像をRGBフォーマットに変換し、共有メモリに書き込むことです。

修正後の src/video_stream_node.cpp のコード
cpp
コードをコピーする
#include <rclcpp/rclcpp.hpp>
#include <std_srvs/srv/set_bool.hpp>
#include <std_srvs/srv/trigger.hpp>
#include <opencv2/opencv.hpp>
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#include <chrono>
#include <thread>
#include <mutex>
#include <ctime>
#include <iomanip>

#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080

struct SharedMemory {
  unsigned long serial_number;
  int width;
  int height;
  unsigned char data[MAX_HEIGHT * MAX_WIDTH * 3];  // 3 channels for RGB888 format
};

class VideoStreamNode : public rclcpp::Node {
public:
  VideoStreamNode() : Node("video_stream_node"), streaming_(false), recording_(false) {
    // ROS2 Services
    stream_service_ = this->create_service<std_srvs::srv::SetBool>(
      "set_streaming", std::bind(&VideoStreamNode::handle_streaming, this, std::placeholders::_1, std::placeholders::_2));
    record_service_ = this->create_service<std_srvs::srv::SetBool>(
      "set_recording", std::bind(&VideoStreamNode::handle_recording, this, std::placeholders::_1, std::placeholders::_2));
    capture_service_ = this->create_service<std_srvs::srv::Trigger>(
      "capture_frame", std::bind(&VideoStreamNode::handle_capture, this, std::placeholders::_1, std::placeholders::_2));

    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_CREAT | O_RDWR, 0666);
    ftruncate(shm_fd_, sizeof(SharedMemory));
    shared_memory_ = (SharedMemory*) mmap(0, sizeof(SharedMemory), PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
      RCLCPP_ERROR(this->get_logger(), "Shared memory initialization failed.");
      exit(EXIT_FAILURE);
    }

    shared_memory_->serial_number = 0;
    shared_memory_->width = MAX_WIDTH;
    shared_memory_->height = MAX_HEIGHT;

    // GStreamer pipeline for H.264 UDP streaming
    std::string pipeline = "udpsrc port=5000 ! application/x-rtp, payload=96 ! "
                           "rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink";
    cap_.open(pipeline, cv::CAP_GSTREAMER);
    if (!cap_.isOpened()) {
      RCLCPP_ERROR(this->get_logger(), "Unable to open video stream.");
      exit(EXIT_FAILURE);
    }

    // Timer to fetch frames at 30 Hz
    timer_ = this->create_wall_timer(
      std::chrono::milliseconds(33), std::bind(&VideoStreamNode::timer_callback, this));
  }

  ~VideoStreamNode() {
    munmap(shared_memory_, sizeof(SharedMemory));
    shm_unlink("/video_shm");
  }

private:
  void handle_streaming(const std::shared_ptr<std_srvs::srv::SetBool::Request> request,
                        std::shared_ptr<std_srvs::srv::SetBool::Response> response) {
    streaming_ = request->data;
    response->success = true;
    response->message = streaming_ ? "Streaming started" : "Streaming stopped";
  }

  void handle_recording(const std::shared_ptr<std_srvs::srv::SetBool::Request> request,
                        std::shared_ptr<std_srvs::srv::SetBool::Response> response) {
    recording_ = request->data;
    if (recording_) {
      // Create VideoWriter for recording
      std::string timestamp = get_current_time_string();
      std::string filename = output_directory_ + "/Video/" + timestamp + ".mp4";
      writer_.open(filename, cv::VideoWriter::fourcc('M', 'P', '4', 'V'), 30, cv::Size(MAX_WIDTH, MAX_HEIGHT));
    } else {
      writer_.release();
    }
    response->success = true;
    response->message = recording_ ? "Recording started" : "Recording stopped";
  }

  void handle_capture(const std::shared_ptr<std_srvs::srv::Trigger::Request> request,
                      std::shared_ptr<std_srvs::srv::Trigger::Response> response) {
    std::string timestamp = get_current_time_string();
    std::string filename = output_directory_ + "/Photo/" + timestamp + ".jpg";
    cv::imwrite(filename, current_frame_);
    response->success = true;
    response->message = "Captured frame saved as " + filename;
  }

  void timer_callback() {
    if (!streaming_) return;

    std::lock_guard<std::mutex> lock(mutex_);

    cap_ >> current_frame_;
    if (current_frame_.empty()) return;

    // Convert frame to RGB format (if not already in that format)
    cv::cvtColor(current_frame_, current_frame_, cv::COLOR_BGR2RGB);

    // Copy to shared memory in RGB888 format
    shared_memory_->serial_number++;
    shared_memory_->width = current_frame_.cols;
    shared_memory_->height = current_frame_.rows;
    memcpy(shared_memory_->data, current_frame_.data, current_frame_.total() * current_frame_.elemSize());

    // Record the frame if recording is active
    if (recording_ && writer_.isOpened()) {
      writer_ << current_frame_;
    }
  }

  std::string get_current_time_string() {
    auto now = std::chrono::system_clock::now();
    auto now_time_t = std::chrono::system_clock::to_time_t(now);
    auto now_ms = std::chrono::duration_cast<std::chrono::milliseconds>(now.time_since_epoch()) % 1000;

    std::ostringstream timestamp;
    timestamp << std::put_time(std::localtime(&now_time_t), "%Y%m%d-%H%M%S")
              << '.' << std::setw(3) << std::setfill('0') << now_ms.count();
    return timestamp.str();
  }

  int shm_fd_;
  SharedMemory* shared_memory_;
  std::mutex mutex_;
  bool streaming_;
  bool recording_;
  cv::Mat current_frame_;
  cv::VideoCapture cap_;
  cv::VideoWriter writer_;
  rclcpp::TimerBase::SharedPtr timer_;
  rclcpp::Service<std_srvs::srv::SetBool>::SharedPtr stream_service_;
  rclcpp::Service<std_srvs::srv::SetBool>::SharedPtr record_service_;
  rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr capture_service_;
  std::string output_directory_ = "/your/output/directory";
};

int main(int argc, char *argv[]) {
  rclcpp::init(argc, argv);
  rclcpp::spin(std::make_shared<VideoStreamNode>());
  rclcpp::shutdown();
  return 0;
}
修正点の説明：
共有メモリのdata部分を、RGB888形式（3チャンネル）に合わせて、1ピクセルあたり3バイトになるように変更しました。
OpenCVで受け取ったフレームをcv::cvtColorでRGBに変換し、直接共有メモリにコピーします。
2. 読み込み側（Qt5ウィジェット）の修正
OpenCVを使わずに、直接共有メモリから読み取ったRGB888データをQImageとして表示するようにします。

修正後の viewer_widget.cpp
cpp
コードをコピーする
#include "viewer_widget.h"
#include <QPainter>
#include <QImage>
#include <QTimer>

ViewerWidget::ViewerWidget(QWidget *parent) : QWidget(parent) {
    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_RDONLY, 0666);
    if (shm_fd_ == -1) {
        throw std::runtime_error("Failed to open shared memory.");
    }

    shared_memory_ = (SharedMemory*)mmap(0, sizeof(SharedMemory), PROT_READ, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
        throw std::runtime_error("Failed to map shared memory.");
    }

    // Timer for updating the frame at 30Hz
    timer_ = new QTimer(this);
    connect(timer_, &QTimer::timeout, this, &ViewerWidget::updateFrame);
    timer_->start(33);  // 30Hz = 1000ms / 30 = 33ms
}

ViewerWidget::~ViewerWidget() {
    munmap(shared_memory_, sizeof(SharedMemory));
    close(shm_fd_);
}

void ViewerWidget::updateFrame() {
    std::lock_guard<std::mutex> lock(mutex_);

    // Ensure the shared memory has valid data
    if (shared_memory_->width > 0 && shared_memory_->height > 0) {
        // Create QImage from shared memory data (assuming RGB888 format)
        current_image_ = QImage(shared_memory_->data, shared_memory_->width, shared_memory_->height, QImage::Format_RGB888);

        // Update the display
        update();
    }
}

void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_image_.isNull()) return;

    // Paint the image onto the widget
    QPainter painter(this);
    painter.drawImage(0, 0, current_image_.scaled(this->size(), Qt::KeepAspectRatio));
}
viewer_widget.h
特に変更はありませんが、以下に示します。

cpp
コードをコピーする
#ifndef VIEWER_WIDGET_H
#define VIEWER_WIDGET_H

#include <QWidget>
#include <QTimer>
#include <QImage>
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#include <mutex>

#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080

struct SharedMemory {
    unsigned long serial_number;
    int width;
    int height;
    unsigned char data[MAX_HEIGHT * MAX_WIDTH * 3];  // RGB888 format
};

class ViewerWidget : public QWidget {
    Q_OBJECT

public:
    explicit ViewerWidget(QWidget *parent = nullptr);
    ~ViewerWidget();

protected:
    void paintEvent(QPaintEvent *event) override;

private slots:
    void updateFrame();

private:
    QTimer *timer_;
    QImage current_image_;
    int shm_fd_;
    SharedMemory *shared_memory_;
    std::mutex mutex_;
};

#endif // VIEWER_WIDGET_H
修正点の説明：
共有メモリから取得したRGB888データを直接QImageに渡して表示します。
OpenCVは使わずに、QImage::Format_RGB888に合わせたデータをそのまま扱います。
3. 実行手順
書き込み側（ROS2ノード）のビルドと実行:
bash
コードをコピーする
colcon build --packages-select video_stream_node
source install/setup.bash
ros2 run video_stream_node video_stream_node
読み込み側（Qt5アプリ）のビルドと実行:
bash
コードをコピーする
mkdir build
cd build
cmake ..
make
./shared_memory_viewer
まとめ
この修正により、書き込み側ではQImage::Format_RGB888に対応する形で共有メモリにデータを書き込み、読み込み側ではそのデータを直接QImageに変換して表示する形にしました。これにより、OpenCVを使わずにデータの表示が可能です。





あなた:
共有メモリ読み込み側のqt5 c++ programで、ただしく映像表示することができません
ChatGPT:

ご指摘いただいた点について、映像が正しく表示されない場合の原因と、その解決策について考えます。

メモリレイアウトの不一致：共有メモリに書き込まれたデータが正しくQImageに変換されていない可能性があります。特に、行のピッチ（1行あたりのバイト数）が正しく扱われていないことが考えられます。

共有メモリのサイズ：MAX_WIDTH、MAX_HEIGHTがフレームの解像度を超えている場合、メモリ上のレイアウトと実際の画像データのレイアウトが異なるため、描画に問題が生じます。

QImageの生成方法：QImageを作成する際に、ピッチ（1行の幅）を指定していない場合、誤った表示がされる可能性があります。

この問題を解決するために、以下の手順でプログラムを修正します。

修正後の viewer_widget.cpp
まず、共有メモリから画像データを正しくQImageに渡すため、QImageを生成する際に各行のピッチ（バイト幅）を明示的に指定します。

修正コード
cpp
コードをコピーする
#include "viewer_widget.h"
#include <QPainter>
#include <QImage>
#include <QTimer>

ViewerWidget::ViewerWidget(QWidget *parent) : QWidget(parent) {
    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_RDONLY, 0666);
    if (shm_fd_ == -1) {
        throw std::runtime_error("Failed to open shared memory.");
    }

    shared_memory_ = (SharedMemory*)mmap(0, sizeof(SharedMemory), PROT_READ, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
        throw std::runtime_error("Failed to map shared memory.");
    }

    // Timer for updating the frame at 30Hz
    timer_ = new QTimer(this);
    connect(timer_, &QTimer::timeout, this, &ViewerWidget::updateFrame);
    timer_->start(33);  // 30Hz = 1000ms / 30 = 33ms
}

ViewerWidget::~ViewerWidget() {
    munmap(shared_memory_, sizeof(SharedMemory));
    close(shm_fd_);
}

void ViewerWidget::updateFrame() {
    std::lock_guard<std::mutex> lock(mutex_);

    // Ensure the shared memory has valid data
    if (shared_memory_->width > 0 && shared_memory_->height > 0) {
        // Create QImage from shared memory data (RGB888 format)
        current_image_ = QImage(shared_memory_->data, 
                                shared_memory_->width, 
                                shared_memory_->height, 
                                shared_memory_->width * 3,  // Bytes per line (3 bytes per pixel for RGB888)
                                QImage::Format_RGB888);

        // Update the display
        update();
    }
}

void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_image_.isNull()) return;

    // Paint the image onto the widget
    QPainter painter(this);
    painter.drawImage(0, 0, current_image_.scaled(this->size(), Qt::KeepAspectRatio));
}
重要な修正点：
QImageの作成:

QImageのコンストラクタで、1行あたりのバイト数（bytesPerLine）をshared_memory_->width * 3として明示的に指定しました。これにより、1ピクセルあたり3バイト（RGB888形式）であることを示しています。これが誤っていると、画像の行のピッチがずれて、表示がおかしくなります。
shared_memory_->width * 3:

shared_memory_->width * 3は、1行の幅に対応するバイト数を計算しています（幅×3チャンネル（RGB））。QImageはこの値を使って画像を正しく解釈します。
その他のチェックポイント
解像度の確認: 共有メモリに書き込まれる画像の幅と高さ（shared_memory_->widthとshared_memory_->height）が実際に送られている画像と一致しているか確認してください。正しい値が設定されていることが重要です。

共有メモリの書き込み側: video_stream_node.cppで書き込む際のフォーマットが正しくRGB888に変換されているか確認してください。具体的には、cv::cvtColor(current_frame_, current_frame_, cv::COLOR_BGR2RGB)が機能しているかです。

まとめ
この修正により、QImageの行ピッチ（bytesPerLine）が正しく指定され、映像が正しく表示されるようになります。





あなた:
QWidgetのウィンドウの右側に白い縦線が表示されるだけです
映像は表示されません
ChatGPT:

問題の症状から考えると、QImageのデータの処理に問題がある可能性が高いです。ウィジェットが白い縦線だけを表示する原因として、以下の点が考えられます。

共有メモリのデータの不一致：QImageに渡すデータが正しい形式・サイズでない場合、表示が崩れたり何も表示されないことがあります。
QImageのdataのアライメント：QImageにデータを渡す際、行のアライメントが正しく処理されていない可能性があります。
以下に、いくつかの改善と確認のポイントを挙げます。

1. QImageのデータアライメントの修正
QImageに直接メモリマップされた共有メモリからデータを渡していますが、QImageはアライメントに依存するため、そのままデータを使用すると問題が生じることがあります。この場合、QImageに対してメモリの内容をコピーすることで安全にデータを渡すことができます。

次に、共有メモリのデータをコピーしてQImageを作成する方法を試してみましょう。

修正後の viewer_widget.cpp のコード
cpp
コードをコピーする
#include "viewer_widget.h"
#include <QPainter>
#include <QImage>
#include <QTimer>

ViewerWidget::ViewerWidget(QWidget *parent) : QWidget(parent) {
    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_RDONLY, 0666);
    if (shm_fd_ == -1) {
        throw std::runtime_error("Failed to open shared memory.");
    }

    shared_memory_ = (SharedMemory*)mmap(0, sizeof(SharedMemory), PROT_READ, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
        throw std::runtime_error("Failed to map shared memory.");
    }

    // Timer for updating the frame at 30Hz
    timer_ = new QTimer(this);
    connect(timer_, &QTimer::timeout, this, &ViewerWidget::updateFrame);
    timer_->start(33);  // 30Hz = 1000ms / 30 = 33ms
}

ViewerWidget::~ViewerWidget() {
    munmap(shared_memory_, sizeof(SharedMemory));
    close(shm_fd_);
}

void ViewerWidget::updateFrame() {
    std::lock_guard<std::mutex> lock(mutex_);

    // Ensure the shared memory has valid data
    if (shared_memory_->width > 0 && shared_memory_->height > 0) {
        // Create a new QImage from a local copy of the shared memory data
        unsigned char *local_data = new unsigned char[shared_memory_->width * shared_memory_->height * 3]; // 3 bytes per pixel
        memcpy(local_data, shared_memory_->data, shared_memory_->width * shared_memory_->height * 3);

        current_image_ = QImage(local_data, 
                                shared_memory_->width, 
                                shared_memory_->height, 
                                shared_memory_->width * 3,  // Bytes per line (3 bytes per pixel for RGB888)
                                QImage::Format_RGB888);

        // Since QImage does not take ownership of raw data by default, we need to manage memory
        current_image_ = current_image_.copy();  // Make a deep copy of the image

        // Clean up the temporary buffer
        delete[] local_data;

        // Update the display
        update();
    }
}

void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_image_.isNull()) return;

    // Paint the image onto the widget
    QPainter painter(this);
    painter.drawImage(0, 0, current_image_.scaled(this->size(), Qt::KeepAspectRatio));
}
2. 変更点の説明
データのローカルコピー:

shared_memory_->dataからローカルにコピーしたバッファ（local_data）を使ってQImageを作成しています。この手順により、QImageのメモリアライメントやデータの破壊などを避けることができます。
QImage::copy():

QImageはデフォルトで外部バッファに対してポインタ参照しますが、今回はcopy()メソッドで、データをQImage内部にコピーして、参照外しの問題を防いでいます。
一時的なバッファのクリーンアップ:

local_dataはnewで動的に割り当てているので、使用後にdelete[]で明示的にメモリを解放しています。
3. その他の確認事項
共有メモリのサイズ：
共有メモリに割り当てたサイズが、正しく設定されているか確認してください。現在の設定では、1920x1080の解像度の画像データを格納しています。解像度に合わないデータを書き込んでいると、表示が正しく行われません。
解像度：
shared_memory_->width と shared_memory_->height が0の場合、何も表示されないか、正しくない解像度で表示されている可能性があります。実際のデータが書き込まれているか確認してください。
4. まとめ
この修正により、共有メモリから安全に画像データを読み込み、QImageとして表示できるはずです。QImageはデータの所有権を保持しないので、copy()でQImageにデータを保持させています。もしこれでも問題が解決しない場合、データのフォーマットや共有メモリに書き込まれている内容に問題があるかもしれないので、その部分も確認する必要があります。





あなた:
共有メモリのデータ構造を以下のようにして、image dataをQImage::Format_RGB888形式で格納してください

#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080
struct
{
uint32 mSerialNumber;
int16 mWidth;
int16 mHeight;
uint32 mImageData[MAX_WIDTH][MAX_HEIGHT];
};
ChatGPT:

指定された共有メモリ構造に基づいて、QImage::Format_RGB888形式で画像データを格納および表示するための修正を行います。以下は、そのためのアプローチです。

まず、QImage::Format_RGB888は、1ピクセルにつき3バイト（24ビット、R、G、Bの各8ビット）で表されます。ですが、ユーザーが指定したデータ構造では、uint32 mImageData[MAX_WIDTH][MAX_HEIGHT]としており、各ピクセルが32ビット（4バイト）で表されているので、これが何を意味するのか考慮します。

一般的なパターンとして、32ビットのピクセルデータは、以下の形式でデータを格納することが多いです：

最下位8ビット：青（B）
次の8ビット：緑（G）
次の8ビット：赤（R）
最上位8ビット：アルファ（A）
しかし、今回はアルファチャネルは無視し、RGBの3つの8ビット部分のみを使用してQImageを生成することを目指します。

1. 共有メモリへのデータの書き込み側の修正
書き込み側の変更：video_stream_node.cpp
指定された構造に従い、32ビットのピクセルデータに変換して格納します。画像データがOpenCVなどで扱われる際、BGR形式で保持されていることが多いので、これをRGB888に変換し、さらに32ビットのピクセルデータに詰めます。

cpp
コードをコピーする
#include <rclcpp/rclcpp.hpp>
#include <std_srvs/srv/set_bool.hpp>
#include <std_srvs/srv/trigger.hpp>
#include <opencv2/opencv.hpp>
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#include <chrono>
#include <thread>
#include <mutex>
#include <ctime>
#include <iomanip>

#define MAX_WIDTH 1920
#define MAX_HEIGHT 1080

struct SharedMemory {
    uint32_t mSerialNumber;
    int16_t mWidth;
    int16_t mHeight;
    uint32_t mImageData[MAX_HEIGHT][MAX_WIDTH];  // Storing as 32-bit per pixel
};

class VideoStreamNode : public rclcpp::Node {
public:
    VideoStreamNode() : Node("video_stream_node"), streaming_(false), recording_(false) {
        // ROS2 Services
        stream_service_ = this->create_service<std_srvs::srv::SetBool>(
            "set_streaming", std::bind(&VideoStreamNode::handle_streaming, this, std::placeholders::_1, std::placeholders::_2));
        record_service_ = this->create_service<std_srvs::srv::SetBool>(
            "set_recording", std::bind(&VideoStreamNode::handle_recording, this, std::placeholders::_1, std::placeholders::_2));
        capture_service_ = this->create_service<std_srvs::srv::Trigger>(
            "capture_frame", std::bind(&VideoStreamNode::handle_capture, this, std::placeholders::_1, std::placeholders::_2));

        // Initialize shared memory
        shm_fd_ = shm_open("/video_shm", O_CREAT | O_RDWR, 0666);
        ftruncate(shm_fd_, sizeof(SharedMemory));
        shared_memory_ = (SharedMemory*)mmap(0, sizeof(SharedMemory), PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd_, 0);
        if (shared_memory_ == MAP_FAILED) {
            RCLCPP_ERROR(this->get_logger(), "Shared memory initialization failed.");
            exit(EXIT_FAILURE);
        }

        shared_memory_->mSerialNumber = 0;
        shared_memory_->mWidth = MAX_WIDTH;
        shared_memory_->mHeight = MAX_HEIGHT;

        // GStreamer pipeline for H.264 UDP streaming
        std::string pipeline = "udpsrc port=5000 ! application/x-rtp, payload=96 ! "
                               "rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink";
        cap_.open(pipeline, cv::CAP_GSTREAMER);
        if (!cap_.isOpened()) {
            RCLCPP_ERROR(this->get_logger(), "Unable to open video stream.");
            exit(EXIT_FAILURE);
        }

        // Timer to fetch frames at 30 Hz
        timer_ = this->create_wall_timer(
            std::chrono::milliseconds(33), std::bind(&VideoStreamNode::timer_callback, this));
    }

    ~VideoStreamNode() {
        munmap(shared_memory_, sizeof(SharedMemory));
        shm_unlink("/video_shm");
    }

private:
    void handle_streaming(const std::shared_ptr<std_srvs::srv::SetBool::Request> request,
                          std::shared_ptr<std_srvs::srv::SetBool::Response> response) {
        streaming_ = request->data;
        response->success = true;
        response->message = streaming_ ? "Streaming started" : "Streaming stopped";
    }

    void handle_recording(const std::shared_ptr<std_srvs::srv::SetBool::Request> request,
                          std::shared_ptr<std_srvs::srv::SetBool::Response> response) {
        recording_ = request->data;
        if (recording_) {
            // Create VideoWriter for recording
            std::string timestamp = get_current_time_string();
            std::string filename = output_directory_ + "/Video/" + timestamp + ".mp4";
            writer_.open(filename, cv::VideoWriter::fourcc('M', 'P', '4', 'V'), 30, cv::Size(MAX_WIDTH, MAX_HEIGHT));
        } else {
            writer_.release();
        }
        response->success = true;
        response->message = recording_ ? "Recording started" : "Recording stopped";
    }

    void handle_capture(const std::shared_ptr<std_srvs::srv::Trigger::Request> request,
                        std::shared_ptr<std_srvs::srv::Trigger::Response> response) {
        std::string timestamp = get_current_time_string();
        std::string filename = output_directory_ + "/Photo/" + timestamp + ".jpg";
        cv::imwrite(filename, current_frame_);
        response->success = true;
        response->message = "Captured frame saved as " + filename;
    }

    void timer_callback() {
        if (!streaming_) return;

        std::lock_guard<std::mutex> lock(mutex_);

        cap_ >> current_frame_;
        if (current_frame_.empty()) return;

        // Convert frame to RGB format (if not already in that format)
        cv::cvtColor(current_frame_, current_frame_, cv::COLOR_BGR2RGB);

        // Copy frame data to shared memory in 32-bit format (ARGB, with alpha ignored)
        shared_memory_->mSerialNumber++;
        shared_memory_->mWidth = current_frame_.cols;
        shared_memory_->mHeight = current_frame_.rows;

        for (int y = 0; y < current_frame_.rows; ++y) {
            for (int x = 0; x < current_frame_.cols; ++x) {
                cv::Vec3b pixel = current_frame_.at<cv::Vec3b>(y, x);
                uint32_t rgb = (pixel[2] << 16) | (pixel[1] << 8) | (pixel[0]);  // Convert BGR to RGB
                shared_memory_->mImageData[y][x] = rgb;
            }
        }

        // Record the frame if recording is active
        if (recording_ && writer_.isOpened()) {
            writer_ << current_frame_;
        }
    }

    std::string get_current_time_string() {
        auto now = std::chrono::system_clock::now();
        auto now_time_t = std::chrono::system_clock::to_time_t(now);
        auto now_ms = std::chrono::duration_cast<std::chrono::milliseconds>(now.time_since_epoch()) % 1000;

        std::ostringstream timestamp;
        timestamp << std::put_time(std::localtime(&now_time_t), "%Y%m%d-%H%M%S")
                  << '.' << std::setw(3) << std::setfill('0') << now_ms.count();
        return timestamp.str();
    }

    int shm_fd_;
    SharedMemory* shared_memory_;
    std::mutex mutex_;
    bool streaming_;
    bool recording_;
    cv::Mat current_frame_;
    cv::VideoCapture cap_;
    cv::VideoWriter writer_;
    rclcpp::TimerBase::SharedPtr timer_;
    rclcpp::Service<std_srvs::srv::SetBool>::SharedPtr stream_service_;
    rclcpp::Service<std_srvs::srv::SetBool>::SharedPtr record_service_;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr capture_service_;
    std::string output_directory_ = "/your/output/directory";
};

int main(int argc, char *argv[]) {
    rclcpp::init(argc, argv);
    rclcpp::spin(std::make_shared<VideoStreamNode>());
    rclcpp::shutdown();
    return 0;
}
変更点の説明：
mImageDataの構造:
各ピクセルデータを32ビット（4バイト）として格納しています。上位8ビット（アルファ）は使いませんが、残りの下位24ビットにRGBデータを詰めます。
cv::Vec3b pixel = current_frame_.at<cv::Vec3b>(y, x)を使い、BGR形式のOpenCVフレームデータをRGBに変換し、32ビット整数として格納します。
2. 共有メモリの読み込み側（Qtプログラム）の修正
次に、読み込み側（Qt5プログラム）で、この32ビットのピクセルデータを読み込んでQImageとして表示する部分を修正します。

修正後の viewer_widget.cpp
cpp
コードをコピーする
#include "viewer_widget.h"
#include <QPainter>
#include <QImage>
#include <QTimer>

ViewerWidget::ViewerWidget(QWidget *parent) : QWidget(parent) {
    // Initialize shared memory
    shm_fd_ = shm_open("/video_shm", O_RDONLY, 0666);
    if (shm_fd_ == -1) {
        throw std::runtime_error("Failed to open shared memory.");
    }

    shared_memory_ = (SharedMemory*)mmap(0, sizeof(SharedMemory), PROT_READ, MAP_SHARED, shm_fd_, 0);
    if (shared_memory_ == MAP_FAILED) {
        throw std::runtime_error("Failed to map shared memory.");
    }

    // Timer for updating the frame at 30Hz
    timer_ = new QTimer(this);
    connect(timer_, &QTimer::timeout, this, &ViewerWidget::updateFrame);
    timer_->start(33);  // 30Hz = 1000ms / 30 = 33ms
}

ViewerWidget::~ViewerWidget() {
    munmap(shared_memory_, sizeof(SharedMemory));
    close(shm_fd_);
}

void ViewerWidget::updateFrame() {
    std::lock_guard<std::mutex> lock(mutex_);

    // Ensure the shared memory has valid data
    if (shared_memory_->mWidth > 0 && shared_memory_->mHeight > 0) {
        // Create a new QImage from the shared memory data
        QImage image(shared_memory_->mWidth, shared_memory_->mHeight, QImage::Format_RGB888);

        // Copy data from shared memory, ignoring the alpha channel
        for (int y = 0; y < shared_memory_->mHeight; ++y) {
            for (int x = 0; x < shared_memory_->mWidth; ++x) {
                uint32_t pixel = shared_memory_->mImageData[y][x];
                // Extract RGB components from 32-bit value
                uint8_t r = (pixel >> 16) & 0xFF;
                uint8_t g = (pixel >> 8) & 0xFF;
                uint8_t b = pixel & 0xFF;
                image.setPixel(x, y, qRgb(r, g, b));
            }
        }

        current_image_ = image;

        // Update the display
        update();
    }
}

void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_image_.isNull()) return;

    // Paint the image onto the widget
    QPainter painter(this);
    painter.drawImage(0, 0, current_image_.scaled(this->size(), Qt::KeepAspectRatio));
}
変更点の説明：
shared_memory_->mImageData[y][x]:

共有メモリから読み込んだ32ビットのピクセルデータを、RGB成分に分解し、QImageに設定しています。qRgb(r, g, b)を使って、24ビットのRGB値をQImageに渡しています。
アルファチャンネルの無視:

mImageDataには32ビット（RGB+A）が格納されていますが、アルファチャンネル（最上位8ビット）は無視し、RGBのみを使用しています。
まとめ
この修正により、指定された32ビットのピクセルフォーマットに従い、共有メモリへのデータ書き込みと、読み込み時のQImage表示が可能になります。mImageDataは32ビットのRGBデータ（アルファなし）を格納し、Qt側ではこれを正しく読み込んで表示する形になっています。




共有する


あなた:
以下のqt5 c++ codeを、画像表示がQWidgetの中央に来るように調整してもらえませんか

void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_image_.isNull()) return;

    // Paint the image onto the widget
    QPainter painter(this);
    painter.drawImage(0, 0, current_image_.scaled(this->size(), Qt::KeepAspectRatio));
}
ChatGPT:

画像をQWidgetの中央に表示するには、現在のコードのpainter.drawImage()の座標を修正して、中央に描画するようにします。QImage::scaled()で生成された画像のサイズを取得し、それをQWidgetの中央に配置するためのオフセットを計算します。

以下に、修正したコードを示します：

cpp
コードをコピーする
void ViewerWidget::paintEvent(QPaintEvent *event) {
    Q_UNUSED(event);

    if (current_image_.isNull()) return;

    // Paint the image onto the widget
    QPainter painter(this);

    // Scale the image while keeping aspect ratio
    QImage scaled_image = current_image_.scaled(this->size(), Qt::KeepAspectRatio);

    // Calculate top-left position to center the image
    int x = (this->width() - scaled_image.width()) / 2;
    int y = (this->height() - scaled_image.height()) / 2;

    // Draw the image at the calculated position
    painter.drawImage(x, y, scaled_image);
}
変更点:
current_image_.scaled()でスケーリングした画像のサイズを取得し、scaled_imageに格納。
ウィジェットの中央に配置するため、xとyのオフセットを計算。
x = (this->width() - scaled_image.width()) / 2
y = (this->height() - scaled_image.height()) / 2
計算した座標 (x, y) にスケーリングされた画像を描画。
このようにすると、画像はウィジェットの中央に表示されます。

